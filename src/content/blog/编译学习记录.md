---
title: 编译学习笔记
description: 学习编译过程中的记录
pubDate: 2025-9-10
image: /image/compliers.png
categories:
  - tech
tags:
  - 编译器
  - compliers
  - 学习笔记
---

# 前言

本篇博客主要记录我学习 complier 的过程.

主要参考实验:[PKU编译原理实验](https://pku-minic.github.io/online-doc/#/)

主要参考书籍: 《编译器设计(第二版)》、《编译方法、技术与实践》

# 正文

## 9-10

### Lv0. 环境配置

记录一下上一周完成的：根据北大编译实验在线文档完成了Lv0.环境配置
配置了docker源，并且根据docker写了一个启动脚本，我选择了c/c++路线使用cmake进行编译。

脚本如下：

```sh
#!/bin/bash

docker run -it --rm -v $(pwd):/root/compiler maxxing/compiler-dev bash -c "
    cd compiler &&
    cmake -DCMAKE_BUILD_TYPE=Debug -B build &&
    cmake --build build &&
    cd build &&
    bash
"
```

其中第一行是根据 Maxxing 的 docker run 脚本改的启动脚本，使用了 `$(pwd)` 增加脚本的通用性.

后面的内容很容易理解.

### Lv1. main函数

#### Lv1.1. 编译器的结构

编译器通常由以下几个部分组成:

- 前端: 通过词法分析和语法分析, 将源代码解析成抽象语法树 (abstract syntax tree, AST). 通过语义分析, 扫描抽象语法树, 检查其是否存在语义错误.
- 中端: 将抽象语法树转换为中间表示 (intermediate representation, IR), 并在此基础上完成一些机器无关优化.
- 后端: 将中间表示转换为目标平台的汇编代码, 并在此基础上完成一些机器相关优化.

一些英文用语

- 词法分析器 (lexer)
- 语法分析器 (parser)
- 字节流 (byte stream)
- 单词流 (token stream)

词法分析的作用, 是把字节流转换为单词流 (token stream).
语法分析的目的, 按照程序的语法规则, 将输入的 token 流变成程序的 AST.
在语法分析的基础上, 编译器会对 AST 做进一步分析, 以期 “理解” 输入程序的语义, 为之后的 IR 生成做准备.
编译器通常会将 AST 转换为另一种形式的数据结构, 我们把它称作 IR. IR 的抽象层次比 AST 更低, 但又不至于低到汇编代码的程度. 在此基础上, 无论是直接把 IR 进一步转换为汇编代码, 还是在 IR 之上做出一些优化, 都相对更容易.（比较出名的LLVM 里面就有很多IR）
编译器进行的最后一步操作, 就是将 IR 转换为目标代码, 也就是目标指令系统的汇编代码.

#### Lv1.2. 语法/词法分析初见

由于生在一个好时代，我们想要实现一个效率蛮不错的词法/语法分析器，并不需要手写递归下降分析器。不过我先在这里放一个网址，可以以后留着来看 [Kaleidoscopea](https://llvm.org/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.html).
现在的工具可以根据正则表达式和 EBNF 生成词法/语法分析器.

EBNF, 即 [Extended Backus–Naur Form](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form), 扩展巴科斯范式, 可以用来描述编程语言的语法.

## 9-11

示例：

```c
int main() {
  // 忽略我的存在
  return 0;
}
```

的语法用EBNF表示为

```ebnf
CompUnit  ::= FuncDef;

FuncDef   ::= FuncType IDENT "(" ")" Block;
FuncType  ::= "int";

Block     ::= "{" Stmt "}";
Stmt      ::= "return" Number ";";
Number    ::= INT_CONST;
```

通过推导可以得到

```ebnf
"int" IDENT "(" ")" "{" "return" INT_CONST ";" "}"
```

由于活在一个好的时代，在C/C++中，我们可以使用 Flex 和 Bison 来分别生成词法分析器和语法分析器.

- Flex 用来描述 EBNF 中的终结符部分, 也就是描述 token 的形式和种类. 你可以使用正则表达式来描述 token.
- Bison 用来描述 EBNF 本身, 其依赖于 Flex 中的终结符描述. 它会生成一个 LALR parser.

关于 Flex 和 Bison 的学习，Maxxing 推荐参考 [Calc++](https://www.gnu.org/software/bison/manual/html_node/A-Complete-C_002b_002b-Example.html) 这里先行放置一下，等以后过来可以学习一下.

Flex 将会读取 `*.l` 文件中描述的词法规则，Bison 将会读取 `*.y` 文件中描述的语法规则.由于这两个文件是互相依赖的 由于 Flex 和 Bison 生成的 lexer 和 parser 会互相调用, 所以这两个文件里的内容也相互依赖.

这两种后缀的文件的结构都是：

```bison
// 这里写一些选项, 可以控制 Flex/Bison 的某些行为

%{

// 这里写一些全局的代码
// 因为最后要生成 C/C++ 文件, 实现主要逻辑的部分都是用 C/C++ 写的
// 难免会用到头文件, 所以通常头文件和一些全局声明/定义写在这里

%}

// 这里写一些 Flex/Bison 相关的定义
// 对于 Flex, 这里可以定义某个符号对应的正则表达式
// 对于 Bison, 这里可以定义终结符/非终结符的类型

%%

// 这里写 Flex/Bison 的规则描述
// 对于 Flex, 这里写的是 lexer 扫描到某个 token 后做的操作
// 对于 Bison, 这里写的是 parser 遇到某种语法规则后做的操作

%%

// 这里写一些用户自定义的代码
// 比如你希望在生成的 C/C++ 文件里定义一个函数, 做一些辅助工作
// 你同时希望在之前的规则描述里调用你定义的函数
// 那么, 你可以把 C/C++ 的函数定义写在这里, 声明写在文件开头
```

实验中给出的实例代码可以在 [这里](https://pku-minic.github.io/online-doc/#/lv1-main/lexer-parser?id=cc-%e5%ae%9e%e7%8e%b0) 看一看.

在誊抄了lab中的代码后，我也是成功运行了这个简易 “编译器” 了.

当然，运行complier的时候记得参数中文件的地址，比如示例文件中 `hello.c` 在 `../` 中，别忘了修改参数，不然会报错

```sh
compiler: /root/compiler/src/main.cpp:27: int main(int, const char **): Assertion `yyin' failed.
Aborted (core dumped)
```

## 9-12

